{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0bdc44-edf4-4a31-899e-e68e4122ec1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# IMPORTANTE!\n",
    "\n",
    "Cluster usado para a realização do ETL RAW TO BRONZE deve ser do tipo  single user.\n",
    "Cluster do tipo \"Shered Cluster\" vai dar erro no merge caso seja a primeira execução (primeira carga). Esse tipo de cluster faz com que a exeção não seja tratada de forma correta. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07ae922a-7882-4b24-b51a-b883ae1235bd",
     "showTitle": true,
     "title": "Importação/Configurações/Variáveis"
    }
   },
   "outputs": [],
   "source": [
    "#author: Bruno - Last Update: Victor - 2023-11-07 21:33\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import lit, when, row_number, current_timestamp, col, trim, coalesce\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from delta import DeltaTable\n",
    "import datetime\n",
    "\n",
    "# Config do spark para não processar lotes que não contenham dados\n",
    "spark.conf.set(\"spark.sql.streaming.noDataMicroBatches.enabled\", \"false\")\n",
    "\n",
    "# Config de evolução de schema na delta table\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "# Variáveis que são definidas no momento de executar o job que estiver ligado a este script\n",
    "SAVE_LOCATION_PATH = \"abfss://dataprep@sgiatec7cloudfivetran.dfs.core.windows.net/checkpoints\"\n",
    "\n",
    "# Variáveis que são definidas no momento de executar o job que estiver ligado a este script\n",
    "SOURCE_CATALOG_NAME     = dbutils.widgets.get(\"source_catalog_name\") # Nome do catálogo de origem\n",
    "TARGET_CATALOG_NAME     = dbutils.widgets.get(\"target_catalog_name\") # target catalog name\n",
    "TAG_PRODUCT             = dbutils.widgets.get(\"tag_product\") # Nome do produto que se encontra na tag do schema\n",
    "DATABRICKS_SCHEMA_NAME  = dbutils.widgets.get(\"databricks_schema_name\") # target shema name, including db schema\n",
    "DATA_DICTIONARY         = eval(dbutils.widgets.get(\"data_dictionary\")) # data dictionary table located in the source default schema\n",
    "IS_UNIFY_DB_PROCESS     = dbutils.widgets.get(\"is_unify_db_process\") # Se \"True\", vai buscar todos os bancos do mesmo produto, fazer checkpoint em pasta diferente e adicionar coluna _source\n",
    "UNIFY_TABLE_NAME        = dbutils.widgets.get(\"unify_table_name\")\n",
    "IGNORE_TABLES = ['fivetran_audit', 'fivetran_log']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0c51c8-bec1-4c3a-a4ba-364b14afdbe9",
     "showTitle": true,
     "title": "Classe Bronze_layer"
    }
   },
   "outputs": [],
   "source": [
    "class Bronze_layer:\n",
    "    def __init__(self, source_catalog_name:str, target_catalog_name:str, schema_name:str, table_name:str, pk_dict:dict, fk_list:list, save_location_path:str, merge_condition:str, key_columns_list:list) -> None :\n",
    "        self.source_catalog_name = source_catalog_name\n",
    "        self.target_catalog_name = target_catalog_name\n",
    "        self.schema_name = schema_name\n",
    "        self.table_name = table_name\n",
    "        self.pk_dict = pk_dict\n",
    "        self.fk_list = fk_list\n",
    "        self.save_location_path = save_location_path\n",
    "        self.merge_condition = merge_condition\n",
    "        self.key_columns_list = key_columns_list\n",
    "    \n",
    "    def create_table(self, unify_schema=None) -> None:\n",
    "        \"\"\"\n",
    "        Função de criação da tabela de destino na camada bronze utilizando engenharia reversa.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'{datetime.datetime.now()} - INFO - Creating target table: {self.target_catalog_name}.{self.schema_name}.{self.table_name}')\n",
    "\n",
    "        # TODO - Criar coluna Primary key \"_7cloud\", se necessário, que subistituirá a coluna GUID. \n",
    "        columns_id_7cloud = '' \n",
    "\n",
    "        for key, value in self.pk_dict.items():\n",
    "            if len(self.pk_dict) == 1:\n",
    "                if value not in ['int', 'bigint']:\n",
    "                    columns_id_7cloud += key + '_7cloud BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1),'\n",
    "        \n",
    "\n",
    "        # TODO - Coletar os metadados da tabela de origem para criar o schema da tabela de destino\n",
    "        schema = spark.table(f'{self.source_catalog_name}.{self.schema_name}.{self.table_name}').schema\n",
    "\n",
    "\n",
    "        # TODO - Limpar os metadados\n",
    "        lista_colunas = [] \n",
    "\n",
    "        for coluna in schema:\n",
    "            coluna = str(coluna)\n",
    "            lista_colunas.append(\n",
    "                coluna[13:].replace(\"DecimalType\", \"Decimal\")\n",
    "                                    .replace(\"',\", \"\")\n",
    "                                    .replace(\"Type()\", \"\")\n",
    "                                    .replace(\", True)\", \"\")\n",
    "                                    .replace(\", False)\", \"\")\n",
    "            )\n",
    "\n",
    "        lista_colunas.remove('_fivetran_synced Timestamp')\n",
    "        try:\n",
    "            lista_colunas.remove('_fivetran_deleted Byte')\n",
    "        except ValueError:\n",
    "            lista_colunas.remove('_fivetran_deleted Boolean')\n",
    "\n",
    "        \n",
    "        # ------------------------------------------------------------------------------------------------------------\n",
    "        #                                       Criação Das Colunas Na Tabela Destino\n",
    "        # ------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # TODO - Criar um dicionário contendo os metadados da tabela\n",
    "        dict_type_columns = dict(subString.lower().split(\" \") for subString in lista_colunas)\n",
    "        \n",
    "        # TODO - Verificar se é necessário adicionar a coluna _source\n",
    "        if '_source' in self.key_columns_list:\n",
    "            self.key_columns_list.remove('_source')\n",
    "            str_columns = '_source STRING,'\n",
    "        else:\n",
    "            str_columns = ''\n",
    "\n",
    "        # TODO - Unir as colunas Primary Kkey e Foreign Key\n",
    "        self.key_columns_list = list(set(self.key_columns_list + self.fk_list))\n",
    "\n",
    "        # TODO - Adicionar colunas chave no início da tabela\n",
    "        for column in self.key_columns_list:\n",
    "            str_columns += column + ' ' + dict_type_columns[column] + ','\n",
    "            dict_type_columns.pop(column)\n",
    "        \n",
    "        # TODO - Adicionar colunas \"comuns\" ao final da tabela\n",
    "        for key, value in dict_type_columns.items():\n",
    "            str_columns += key + ' ' + value + ','\n",
    "\n",
    "        str_columns = str_columns[:-1]\n",
    "\n",
    "        # TODO - Criar o schema de destino\n",
    "        if unify_schema is None:\n",
    "            spark.sql(f'CREATE SCHEMA IF NOT EXISTS {self.target_catalog_name}.{self.schema_name}')\n",
    "            \n",
    "            # TODO - Aterar o proprietário do schema para `UnityAdmin`\n",
    "            spark.sql(f\"ALTER SCHEMA {self.target_catalog_name}.{self.schema_name} SET OWNER TO UnityAdmin\")\n",
    "            \n",
    "            # TODO - Criar a tabela de destino com os metadados da tabela de origem\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE {self.target_catalog_name}.{self.schema_name}.{self.table_name}\n",
    "                (\n",
    "                    {columns_id_7cloud}\n",
    "                    {str_columns}\n",
    "                )\n",
    "                TBLPROPERTIES (\n",
    "                    delta.enableDeletionVectors = true,\n",
    "                    delta.enableChangeDataFeed = true,\n",
    "                    delta.autoOptimize.autoCompact=true,\n",
    "                    delta.autoOptimize.optimizeWrite=true,\n",
    "                    delta.columnMapping.mode = 'name',\n",
    "                    delta.minReaderVersion = '3',\n",
    "                    delta.minWriterVersion = '7',\n",
    "                    delta.feature.allowColumnDefaults = 'supported'\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "            # TODO - Aterar o proprietário da tabela para `UnityAdmin`\n",
    "            spark.sql(f\"ALTER TABLE {self.target_catalog_name}.{self.schema_name}.{self.table_name} SET OWNER TO UnityAdmin\")\n",
    "        else:\n",
    "            spark.sql(f'CREATE SCHEMA IF NOT EXISTS {self.target_catalog_name}.{unify_schema}')\n",
    "            # TODO - Aterar o proprietário do schema para `UnityAdmin`\n",
    "            spark.sql(f\"ALTER SCHEMA {self.target_catalog_name}.{unify_schema} SET OWNER TO UnityAdmin\")\n",
    "            \n",
    "            # TODO - Criar a tabela de destino com os metadados da tabela de origem\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE {self.target_catalog_name}.{unify_schema}.{self.table_name}\n",
    "                (\n",
    "                    {columns_id_7cloud}\n",
    "                    {str_columns}\n",
    "                )\n",
    "                TBLPROPERTIES (\n",
    "                    delta.enableDeletionVectors = true,\n",
    "                    delta.enableChangeDataFeed = true, \n",
    "                    delta.autoOptimize.autoCompact=true,\n",
    "                    delta.autoOptimize.optimizeWrite=true,\n",
    "                    delta.columnMapping.mode = 'name',\n",
    "                    delta.minReaderVersion = '3',\n",
    "                    delta.minWriterVersion = '7',\n",
    "                    delta.feature.allowColumnDefaults = 'supported'\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "            # TODO - Aterar o proprietário da tabela para `UnityAdmin`\n",
    "            spark.sql(f\"ALTER TABLE {self.target_catalog_name}.{unify_schema}.{self.table_name} SET OWNER TO UnityAdmin\")\n",
    "    \n",
    "    def erro_except(self, type_error, unify_schema=None) -> None:\n",
    "        \"\"\"\n",
    "        Função para tratamento de erros ocorrido durante o streaming. Os tipos de erros tratados são:\n",
    "            - CDF: Erro de configuração do Change Data Feed. Ocorre quando a tabela que está sendo estrimada não possui o CDF habilitado.\n",
    "            - CHECKPOINT: Erro de checkpoint. Ocorre quando o checkpoint não pertence a tabela raw que está sendo processada.\n",
    "        \n",
    "        :param type_error: Erro ocorrido durante o streaming.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "            \n",
    "        type_error_str = str(type_error)\n",
    "        cdf_error = True if 'delta.enableChangeDataFeed=true' in type_error_str else False\n",
    "        checkpoint_error = True if 'delete your streaming query checkpoint' in type_error_str else False\n",
    "\n",
    "        if cdf_error:\n",
    "            print(f'{datetime.datetime.now()} - WARNING - Solving CDF Error')\n",
    "            \n",
    "            # TODO - Remover possíveis checkpoints da antiga tabela \n",
    "            if unify_schema is None:\n",
    "                dbutils.fs.rm(f\"{self.save_location_path}/{self.source_catalog_name}/{self.schema_name}/{self.table_name}\", True)\n",
    "            else:\n",
    "                dbutils.fs.rm(f\"{self.save_location_path}/unified/{self.source_catalog_name}/{self.schema_name}/{self.table_name}\", True)\n",
    "\n",
    "            # TODO - Habilitar o CDF da tabela com erro\n",
    "            spark.sql(f\"\"\"\n",
    "                    ALTER TABLE {self.source_catalog_name}.{self.schema_name}.{self.table_name}\n",
    "                    SET TBLPROPERTIES (\n",
    "                        delta.enableChangeDataFeed = true,\n",
    "                        delta.autoOptimize.autoCompact=true,\n",
    "                        delta.autoOptimize.optimizeWrite=true\n",
    "                    )\n",
    "            \"\"\")\n",
    "                \n",
    "        elif checkpoint_error:\n",
    "            print(f'{datetime.datetime.now()} - WARNING - Solving Checkpoint Error')\n",
    "            if unify_schema is None:\n",
    "                dbutils.fs.rm(f\"{self.save_location_path}/{self.source_catalog_name}/{self.schema_name}/{self.table_name}\", True)\n",
    "            \n",
    "                # Delete Registros que possivelmente estejam na camada Bronze e não foram apagados por causa de ressincronização \n",
    "                print(f'{datetime.datetime.now()} - WARNING - Deleting Pendenting Records')\n",
    "                spark.sql(f\"\"\"\n",
    "                    DELETE FROM {self.target_catalog_name}.{self.schema_name}.{self.table_name} AS target\n",
    "                    WHERE NOT EXISTS (SELECT * FROM {self.source_catalog_name}.{self.schema_name}.{self.table_name} AS source\n",
    "                                    WHERE {self.merge_condition})\n",
    "                \"\"\")\n",
    "            else:\n",
    "                dbutils.fs.rm(f\"{self.save_location_path}/unified/{self.source_catalog_name}/{self.schema_name}/{self.table_name}\", True)\n",
    "            \n",
    "                # Delete Registros que possivelmente estejam na camada Bronze e não foram apagados por causa de ressincronização \n",
    "                print(f'{datetime.datetime.now()} - WARNING - Deleting Pendenting Records')\n",
    "                spark.sql(f\"\"\"\n",
    "                    DELETE FROM {self.target_catalog_name}.{unify_schema}.{self.table_name} AS target\n",
    "                    WHERE NOT EXISTS (SELECT * FROM {self.source_catalog_name}.{self.schema_name}.{self.table_name} AS source\n",
    "                                    WHERE {self.merge_condition[:-36]}) AND target._source = '{self.source_catalog_name}.{self.schema_name}'\n",
    "                \"\"\")\n",
    "\n",
    "        else:\n",
    "            raise type_error\n",
    "\n",
    "    def deduplicate_records(self, batch_df:DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Função para deduplicação de registros, onde a última versão do registro será mantida.\n",
    "\n",
    "        :param batch_df: DataFrame com os registros a serem deduplicados\n",
    "        :return: DataFrame com os registros deduplicados\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # TODO - Particionar o DataFrame a partir de uma lista de colunas chave para pegar somente a última versão do registro\n",
    "        window = Window.partitionBy(self.key_columns_list).orderBy(col(\"_commit_timestamp\").desc())\n",
    "        batch_df = batch_df.withColumn(\"rank\", row_number().over(window)) \\\n",
    "                            .where('rank = 1')\n",
    "        \n",
    "        # TODO - Criar uma  coluna com o tipo de operação e remover colunas desnecessárias\n",
    "        batch_df = batch_df.withColumn('_operation', when(batch_df._fivetran_deleted == 1, lit('D')) \\\n",
    "                                .when(batch_df._fivetran_deleted == True, lit('D')) \\\n",
    "                                .when(batch_df._change_type == 'delete', lit('D')) \\\n",
    "                                .when((batch_df._change_type == 'update_postimage') & (batch_df._fivetran_deleted == 0), lit('U')) \\\n",
    "                                .when((batch_df._change_type == 'update_postimage') & (batch_df._fivetran_deleted == False), lit('U')) \\\n",
    "                                .when(batch_df._change_type == 'insert', lit('I')) \\\n",
    "                                .otherwise('NULL')) \\\n",
    "                            .withColumn('_bronzeUpdateDate', current_timestamp()) \\\n",
    "                            .drop('_fivetran_deleted', '_fivetran_synced', '_change_type', '_commit_version', '_commit_timestamp', 'rank')\n",
    "        return batch_df\n",
    "\n",
    "    def merge_changes(self, batch_df:DataFrame, batch_id:int = None, unify_schema:str = None) -> None:\n",
    "        \"\"\"\n",
    "        Função para mesclar mudanças na tabela da camada bronze.\n",
    "        \n",
    "        :param batch_df: DataFrame com os registros a serem mesclados\n",
    "        :param batch_id: ID do batch\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO - Criar um lista de colunas Primary Key para serem utilizadas no particionamento do DataFrame\n",
    "        for key, value in self.pk_dict.items():\n",
    "            self.key_columns_list.append(key)\n",
    "        \n",
    "\n",
    "        # TODO - Deduplicidade e limpeza do batch\n",
    "        batch_df = self.deduplicate_records(batch_df)\n",
    "\n",
    "\n",
    "        # TODO - Tenta ler a tabela de destino. Cria a tabela caso não exista\n",
    "        if unify_schema is None:\n",
    "            try:\n",
    "                delta_df = DeltaTable.forName(spark, f'{self.target_catalog_name}.{self.schema_name}.{self.table_name}')\n",
    "            except Exception as e:\n",
    "                self.create_table()\n",
    "                delta_df = DeltaTable.forName(spark, f'{self.target_catalog_name}.{self.schema_name}.{self.table_name}')\n",
    "        else:\n",
    "            try:\n",
    "                delta_df = DeltaTable.forName(spark, f'{self.target_catalog_name}.{unify_schema}.{self.table_name}')\n",
    "            except Exception as e:\n",
    "                self.create_table(unify_schema=unify_schema)\n",
    "                delta_df = DeltaTable.forName(spark, f'{self.target_catalog_name}.{unify_schema}.{self.table_name}')\n",
    "        \n",
    "\n",
    "        # TODO - Execute o MERGE com Evolution Schema\n",
    "        delta_df.alias(\"target\") \\\n",
    "            .merge(batch_df.alias(\"source\"), self.merge_condition) \\\n",
    "            .whenMatchedDelete(\"source._operation = 'D'\") \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll(\"source._operation != 'D'\") \\\n",
    "            .execute()\n",
    "\n",
    "    def streaming_table(self) -> None:\n",
    "        \"\"\"\n",
    "        Função para streaming da tabela usando o Change Data Feed. \n",
    "        Esse streaming chama uma função que resultará em um merge das atualizações da tabela da camada bronze.\n",
    "        \n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        (\n",
    "            spark.readStream\n",
    "                .option(\"readChangeData\", \"true\")\n",
    "                .table(f\"{self.source_catalog_name}.{self.schema_name}.{self.table_name}\")\n",
    "                .filter(col('_change_type').isin([\"insert\",\"delete\",\"update_postimage\"]))\n",
    "            .writeStream\n",
    "                .foreachBatch(self.merge_changes)\n",
    "                .option(\"checkpointLocation\", f\"{self.save_location_path}/{self.source_catalog_name}/{self.schema_name}/{self.table_name}/_checkpoint\")\n",
    "                .trigger(availableNow=True)\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "        )\n",
    "    \n",
    "    def streaming_table_append_only(self, tag_product:str) -> None:\n",
    "        \"\"\"\n",
    "        Função para streaming em modo \"append only\" da tabela usando o Change Data Feed. O destino desse streaming é uma tabela de stage que conterá vários appends, correspondendo a atualizações de tabelas que serão unificadas na camada bronze.\n",
    "\n",
    "        :param tag_product: Tag do produto que servirá para identificar a tabela de destino.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        (\n",
    "            spark.readStream\n",
    "                .option(\"readChangeData\", \"true\")\n",
    "                .table(f\"{self.source_catalog_name}.{self.schema_name}.{self.table_name}\")\n",
    "                .filter(col('_change_type').isin([\"insert\",\"delete\",\"update_postimage\"]))\n",
    "                .withColumn('_source', lit(f'{self.source_catalog_name}.{self.schema_name}')) \\\n",
    "            .writeStream\n",
    "                .option(\"checkpointLocation\", f\"{self.save_location_path}/unified/{self.source_catalog_name}/{self.schema_name}/{self.table_name}/_checkpoint\")\n",
    "                .trigger(availableNow=True)\n",
    "                .table(f\"{self.target_catalog_name}.stage.{tag_product}_{self.table_name}\")\n",
    "            .awaitTermination()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab48cf26-b0b8-4885-a25a-d3ed2e06d843",
     "showTitle": true,
     "title": "Primary key  e Foreing Key"
    }
   },
   "outputs": [],
   "source": [
    "def get_primary_key(source_catalog_name:str, table_name:str, data_dictionary:dict) -> dict:\n",
    "    \"\"\"\n",
    "    Função para pegar a(s) Primary Key(s) da tabela e seu tipo de dado usando um dicionário de dados.\n",
    "    Ex: {'id': 'bigint'}\n",
    "\n",
    "    :param source_catalog_name: Nome do catálogo da origem\n",
    "    :param table_name: Nome da tabela\n",
    "    :param data_dictionary: Dicionário com as informações do Data Dictionary da tabela\n",
    "    :return: Dicionário com a Primary Key\n",
    "    \"\"\"\n",
    "    \n",
    "    pk_df = spark.sql(f\"\"\"\n",
    "        SELECT ColunaNome, ColunaTipo\n",
    "        FROM {source_catalog_name}.default.{data_dictionary[\"dict_name\"]}\n",
    "        WHERE DatabaseName = '{data_dictionary[\"database\"]}' \n",
    "            AND SchemaNome = '{data_dictionary[\"schema\"]}' \n",
    "            AND TabelaNome = '{table_name}' \n",
    "            AND IsPK = 'PK'\n",
    "    \"\"\")\n",
    "\n",
    "    pk_dict = {row['ColunaNome']: row['ColunaTipo'] for row in pk_df.collect()}\n",
    "    return pk_dict\n",
    "\n",
    "def get_foreing_key(source_catalog_name:str, table_name:str, data_dictionary:dict) -> list:\n",
    "    \"\"\"\n",
    "    Função para pegar a(s) Foreign Key(s) da tabela e seu tipo de dado usando um dicionário de dados.\n",
    "    Ex: ['fk_table_name']\n",
    "\n",
    "    :param source_catalog_name: Nome do catálogo da origem\n",
    "    :param table_name: Nome da tabela\n",
    "    :param data_dictionary: Dicionário com as informações do Data Dictionary da tabela\n",
    "    :return: Lista com as Foreign Keys\n",
    "    \"\"\"\n",
    "\n",
    "    fk_table = spark.sql(f\"\"\"\n",
    "        SELECT ColunaNome\n",
    "        FROM {source_catalog_name}.default.{data_dictionary[\"dict_name\"]}\n",
    "        WHERE DatabaseName = '{data_dictionary[\"database\"]}' \n",
    "            AND SchemaNome = '{data_dictionary[\"schema\"]}' \n",
    "            AND TabelaNome = '{table_name}' \n",
    "            AND trim(FKRef) != ''\n",
    "            AND coalesce(IsPK, '0') = '0'\n",
    "    \"\"\")\n",
    "\n",
    "    fk_list = [row['ColunaNome'] for row in fk_table.collect()]\n",
    "    return fk_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b26561c-9808-4888-a021-a8c96992784a",
     "showTitle": true,
     "title": "Tabelas do Schema"
    }
   },
   "outputs": [],
   "source": [
    "def tables_from_schema(schema_name:str, source_catalog_name:str) -> list:\n",
    "    \"\"\"\n",
    "    Função para pegar uma lista com as tabelas que estão no schema informado.\n",
    "\n",
    "    :param schema_name: Nome do schema\n",
    "    :param source_catalog_name: Nome do catálogo da origem\n",
    "    :return: Lista com as tabelas\n",
    "    \"\"\"\n",
    "\n",
    "    table_name_df = spark.sql(f\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM {source_catalog_name}.information_schema.tables\n",
    "        WHERE table_schema = '{schema_name}'\n",
    "        ORDER BY table_name\n",
    "    \"\"\")\n",
    "\n",
    "    table_list = [row['table_name'] for row in table_name_df.collect()]\n",
    "    return table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6295ac98-1cf3-4dc6-bf61-b8b9a3326922",
     "showTitle": true,
     "title": "Schemas da Tabela"
    }
   },
   "outputs": [],
   "source": [
    "def schemas_from_table(table_name:str, source_catalog_name:str, tag_product:str) ->list:\n",
    "    \"\"\"\n",
    "    Função para pegar uma lista com os schemas que contém a tabela informada.\n",
    "\n",
    "    :param table_name: Nome da tabela\n",
    "    :param source_catalog_name: Nome do catálogo da origem\n",
    "    :param tag_product: Nome do tag do produto\n",
    "    :return: Lista com os schemas\n",
    "    \"\"\"\n",
    "    schemas_df = spark.sql(f\"\"\"\n",
    "        SELECT      table_schema\n",
    "        FROM        {source_catalog_name}.information_schema.tables t\n",
    "        INNER JOIN  {source_catalog_name}.information_schema.schema_tags st     ON st.schema_name = t.table_schema\n",
    "        WHERE       t.table_name = '{table_name}' AND st.tag_name = 'product'   AND st.tag_value = '{tag_product}'\n",
    "        ORDER BY    table_schema     ASC\n",
    "    \"\"\")\n",
    "\n",
    "    schemas_list = [row['table_schema'] for row in schemas_df.collect()]\n",
    "    return schemas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89ef02ca-4b32-4fd5-9cf7-e26fe2915fbe",
     "showTitle": true,
     "title": "Processo em Tabelas Unificadas"
    }
   },
   "outputs": [],
   "source": [
    "def unified_table_process(unify_table_name:str, source_catalog_name:str, target_catalog_name:str, tag_product:str, unify_schema:str, data_dictionary:dict, ignore_tables:str, save_location_path:str) -> None:\n",
    "    \"\"\"\n",
    "    Função para iniciar o processo atualizações de tabelas unificadas.\n",
    "\n",
    "    :param unify_table_name: Nome da tabela unificada\n",
    "    :param source_catalog_name: Nome do catálogo da origem\n",
    "    :param target_catalog_name: Nome do catálogo de destino\n",
    "    :param tag_product: Nome do tag do produto\n",
    "    :param data_dictionary: Dicionário com as informações do data dictionary\n",
    "    :param ignore_tables: Lista com as tabelas que não devem ser enviadas para a camada bronze\n",
    "    :param save_location_path: Caminho onse serão salvos os checkponts\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO - Criar o schema stage para armazenar as tabelas de stage\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog_name}.stage\")\n",
    "    spark.sql(f\"ALTER SCHEMA {target_catalog_name}.stage SET OWNER TO UnityAdmin\")\n",
    "\n",
    "    # TODO - Pegar as primary keys e foreign keys da tabela\n",
    "    pk_dict = get_primary_key(source_catalog_name=source_catalog_name, table_name=unify_table_name, data_dictionary=data_dictionary)\n",
    "    fk_list = get_foreing_key(source_catalog_name=source_catalog_name, table_name=unify_table_name, data_dictionary=data_dictionary)\n",
    "\n",
    "    # TODO - Criar uma variável para armazenar a condição de merge\n",
    "    merge_condition = ''\n",
    "    for key, value in pk_dict.items():\n",
    "        merge_condition += 'target.' + key + ' = source.' + key + ' AND '\n",
    "    merge_condition = merge_condition + \"target._source = source._source\"\n",
    "\n",
    "    # TODO - Pegar uma lista de schemas que contem a tabela\n",
    "    schemas_list = schemas_from_table(unify_table_name, source_catalog_name, tag_product)\n",
    "\n",
    "    # TODO - Iniciar o processo de captura de atualizações de registros\n",
    "    for schema_name in schemas_list:\n",
    "        print(f'{datetime.datetime.now()} - INFO - Starting Stream: {source_catalog_name}.{schema_name}.{unify_table_name}')\n",
    "\n",
    "        table_obj = Bronze_layer(\n",
    "            source_catalog_name = source_catalog_name,\n",
    "            target_catalog_name = target_catalog_name,\n",
    "            table_name = unify_table_name,\n",
    "            schema_name = schema_name,\n",
    "            pk_dict = pk_dict,\n",
    "            fk_list = fk_list,\n",
    "            save_location_path = save_location_path,\n",
    "            merge_condition = merge_condition,\n",
    "            key_columns_list = ['_source']\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            table_obj.streaming_table_append_only(tag_product=tag_product)\n",
    "        except Exception as type_error:\n",
    "            table_obj.erro_except(type_error=type_error, unify_schema=unify_schema)\n",
    "            print(f'{datetime.datetime.now()} - WARNING - Restarting streaming table')\n",
    "            table_obj.streaming_table_append_only(tag_product=tag_product)\n",
    "\n",
    "        print(f'{datetime.datetime.now()} - INFO - Finished Stream: {source_catalog_name}.{schema_name}.{unify_table_name}')\n",
    "\n",
    "\n",
    "        # TODO - Iniciar o processo de merge de registros caso seja o último schema\n",
    "        if schema_name == schemas_list[-1]:\n",
    "            unify_stage_df = spark.table(f\"{table_obj.target_catalog_name}.stage.{tag_product}_{table_obj.table_name}\")\n",
    "\n",
    "            table_obj.merge_changes(batch_df=unify_stage_df, unify_schema=unify_schema)\n",
    "\n",
    "            # TODO - Apagar a tabela de stage no fim do processo.\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table_obj.target_catalog_name}.stage.{tag_product}_{table_obj.table_name}\") \n",
    "\n",
    "    \n",
    "    # TODO - Verificar a integridade das tabelas unificadas\n",
    "    print(f'{datetime.datetime.now()} - INFO - Start Integrity Verification: {unify_schema}_{unify_table_name}')\n",
    "    linha_integridade = []\n",
    "\n",
    "\n",
    "    for schema_name in schemas_list:\n",
    "        # TODO - Realizar a contagem de quantos registros existem na tabela raw e na tabela bronze.\n",
    "        # Caso falhe, o processo irá registrar 0 (zero) para cada uma das colunas\n",
    "        try:\n",
    "            qtd_raw = spark.sql(f\"\"\"\n",
    "                SELECT * FROM {source_catalog_name}.`{schema_name}`.`{unify_table_name}`\n",
    "                WHERE _fivetran_deleted = False OR _fivetran_deleted = 0\n",
    "            \"\"\").count()                                                                                                            \n",
    "\n",
    "            qtd_bronze = spark.sql(f\"\"\"\n",
    "                SELECT * FROM {target_catalog_name}.`{unify_schema}`.`{unify_table_name}`\n",
    "                WHERE _source = '{source_catalog_name}.{schema_name}'\n",
    "            \"\"\").count()\n",
    "\n",
    "            linha_integridade.append((schema_name, unify_table_name, qtd_raw, qtd_bronze))\n",
    "        except:\n",
    "            linha_integridade.append((schema_name, unify_table_name, 0, 0))\n",
    "\n",
    "    # TODO - Salvar a integridade das tabelas unificadas\n",
    "    schema_integridade = StructType([\n",
    "                    StructField('nome_schema', StringType(), True),\n",
    "                    StructField('nome_table', StringType(), True),\n",
    "                    StructField('qtd_raw', IntegerType(), True),\n",
    "                    StructField('qtd_bronze', IntegerType(), True)\n",
    "                    ])\n",
    "\n",
    "    df_integridade = spark.createDataFrame(linha_integridade, schema_integridade)\n",
    "    df_integridade.write.mode('overwrite').saveAsTable(f'{target_catalog_name}.default.integridade_{unify_table_name}')\n",
    "\n",
    "    print(f'{datetime.datetime.now()} - INFO - End of Integrity Verification: {unify_schema}_{unify_table_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e90460d-d5f4-4ac8-ae04-e232fae6b6d9",
     "showTitle": true,
     "title": "Processo Baseado em Schema"
    }
   },
   "outputs": [],
   "source": [
    "def schema_table_process(source_catalog_name:str, target_catalog_name:str, databricks_schema_name:str, data_dictionary:dict, ignore_tables:str, save_location_path:str) -> None:\n",
    "    \"\"\"\n",
    "    Função para iniciar o processo de ETL de tabelas que estão dentro de um schema específico.\n",
    "    \n",
    "        :param source_catalog_name: Nome do catálogo da origem\n",
    "        :param target_catalog_name: Nome do catálogo de destino\n",
    "        :param databricks_schema_name: Nome do schema do Databricks que será processado\n",
    "        :param data_dictionary: Dicionário com as informações do dicionário de dados das tabelas do produto\n",
    "        :param ignore_tables: Lista com as tabelas que não devem ser enviadas para a camada bronze, como tabelas de controle do Fivetran. \n",
    "            Ex: ['fivetran_connection', 'fivetran_log']\n",
    "        :param save_location_path: Caminho onde serão salvos os checkponts\n",
    "        :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Usar o catálogo fonte que foi passado como parâmetro no script\n",
    "    table_list = tables_from_schema(schema_name=databricks_schema_name, source_catalog_name=source_catalog_name)\n",
    "    linha_integridade = []\n",
    "\n",
    "    # Laço de repetição para iniciar o streaming das tabelas que serão enviadas para a camada bronze\n",
    "    for table in table_list:\n",
    "\n",
    "        # Condição se a tabela deve ou não ser enviada para a camada bronze\n",
    "        # EX: Não enviar tabelas de controle do Fivetran (tabela fivetran_audit)\n",
    "        if table not in ignore_tables:\n",
    "            # TODO - Pegar as primary keys e foreign keys da tabela\n",
    "            pk_dict = get_primary_key(source_catalog_name=source_catalog_name, table_name=table, data_dictionary=data_dictionary)\n",
    "            fk_list = get_foreing_key(source_catalog_name=source_catalog_name, table_name=table, data_dictionary=data_dictionary)\n",
    "\n",
    "            # TODO - Criar uma variável para armazenar a condição de merge\n",
    "            merge_condition = ''\n",
    "            for key, value in pk_dict.items():\n",
    "                merge_condition += 'target.' + key + ' = source.' + key + ' AND '\n",
    "            merge_condition = merge_condition[:-5]\n",
    "\n",
    "                \n",
    "            # TODO - Criar o obejeto tabela que será usada no streaming\n",
    "            table_obj = Bronze_layer(\n",
    "                source_catalog_name=source_catalog_name,\n",
    "                target_catalog_name=target_catalog_name,\n",
    "                schema_name=databricks_schema_name,\n",
    "                table_name=table,\n",
    "                pk_dict=pk_dict,\n",
    "                fk_list=fk_list,\n",
    "                save_location_path=save_location_path,\n",
    "                merge_condition=merge_condition,\n",
    "                key_columns_list=[]\n",
    "            )\n",
    "\n",
    "            # TODO - Iniciar o streaming do objeto. Caso der erro, resolver as exceções e reiniciar o processo de streaming\n",
    "            try:\n",
    "                print(f'{datetime.datetime.now()} - INFO - Starting Stream: {source_catalog_name}.{databricks_schema_name}.{table}')\n",
    "                table_obj.streaming_table()\n",
    "                \n",
    "                # TODO - Realizar a contagem de quantos registros existem na tabela raw e na tabela bronze.\n",
    "                # Caso falhe, o processo irá registrar 0 (zero) para cada uma das colunas\n",
    "                \"\"\"\n",
    "                Nota: o processo de ferificação de integridade foi colocado nesse ponto para ter uma menor latência entre a chegada de dados na camada raw e o ETL para a camada bronze. Dessa forma, a checagem de integridade será realizada logo após a passagem dos dados de cada tabela para a camada bronze, não sendo necessário esperar o fim do ETL de todas as tabelas, como ocorre nas tabelas unificadas. \n",
    "                \"\"\"\n",
    "                try:\n",
    "                    qtd_bronze = spark.table(f'{target_catalog_name}.`{databricks_schema_name}`.`{table}`').count()\n",
    "                    qtd_raw = spark.table(f'{source_catalog_name}.`{databricks_schema_name}`.`{table}`') \\\n",
    "                                    .filter((col('_fivetran_deleted') == False) | (col('_fivetran_deleted') == 0)).count()\n",
    "                                    \n",
    "                    linha_integridade.append((databricks_schema_name, table, qtd_raw, qtd_bronze))\n",
    "                except:\n",
    "                    linha_integridade.append((databricks_schema_name, table, 0, 0))\n",
    "\n",
    "            except Exception as type_error:\n",
    "                table_obj.erro_except(type_error=type_error)\n",
    "                print(f'{datetime.datetime.now()} - WARNING - Restarting streaming table')\n",
    "                table_obj.streaming_table()\n",
    "            \n",
    "            print(f'{datetime.datetime.now()} - INFO - Finished Stream: {source_catalog_name}.{databricks_schema_name}.{table}')\n",
    "\n",
    "\n",
    "    # TODO - Salvar tabela de integridade em modo overwrite\n",
    "    schema_integridade = StructType([\n",
    "                    StructField('nome_schema', StringType(), True),\n",
    "                    StructField('nome_table', StringType(), True),\n",
    "                    StructField('qtd_raw', IntegerType(), True),\n",
    "                    StructField('qtd_bronze', IntegerType(), True)\n",
    "                    ])\n",
    "\n",
    "    df_integridade = spark.createDataFrame(linha_integridade, schema_integridade)\n",
    "    df_integridade.write.mode('overwrite').saveAsTable(f'{target_catalog_name}.default.integridade_{databricks_schema_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77afa752-cb4e-4479-b4df-9b70cd552471",
     "showTitle": true,
     "title": "Início do Processo"
    }
   },
   "outputs": [],
   "source": [
    "if IS_UNIFY_DB_PROCESS == \"True\":\n",
    "    unified_table_process(\n",
    "        unify_table_name        = UNIFY_TABLE_NAME,\n",
    "        source_catalog_name     = SOURCE_CATALOG_NAME,\n",
    "        target_catalog_name     = TARGET_CATALOG_NAME,\n",
    "        tag_product             = TAG_PRODUCT,\n",
    "        unify_schema            = DATABRICKS_SCHEMA_NAME,\n",
    "        data_dictionary         = DATA_DICTIONARY,\n",
    "        ignore_tables           = IGNORE_TABLES,\n",
    "        save_location_path      = SAVE_LOCATION_PATH\n",
    "    )\n",
    "elif IS_UNIFY_DB_PROCESS == \"False\":\n",
    "    schema_table_process(\n",
    "        source_catalog_name     = SOURCE_CATALOG_NAME,\n",
    "        target_catalog_name     = TARGET_CATALOG_NAME,\n",
    "        databricks_schema_name  = DATABRICKS_SCHEMA_NAME,\n",
    "        data_dictionary         = DATA_DICTIONARY,\n",
    "        ignore_tables           = IGNORE_TABLES,\n",
    "        save_location_path      = SAVE_LOCATION_PATH\n",
    "    )\n",
    "else:\n",
    "    assert False, \"A variável IS_UNIFY_DB_PROCESS deve ser 'True' ou 'False'.\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
