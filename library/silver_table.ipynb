{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f40fd201-5dac-497f-9e90-42374712e247",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, current_timestamp, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from delta import DeltaTable\n",
    "import datetime\n",
    "\n",
    "# Config do spark para não processar lotes que não contenham dados\n",
    "spark.conf.set(\"spark.sql.streaming.noDataMicroBatches.enabled\", \"true\")\n",
    "\n",
    "# Config de evolução de schema na delta table\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d33e79b-e43a-4b32-9277-6f4f285b9ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Silver_table:\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, create_table_columns: str, column_merge: dict, merge_condition: str,  delete_condition: str, update_condition: str, insert_condition: str):\n",
    "        self.source_table_name = source_table_name\n",
    "        self.target_table_name = target_table_name\n",
    "        self.create_table_columns = create_table_columns\n",
    "        self.column_merge = column_merge\n",
    "        self.merge_condition = merge_condition\n",
    "        self.delete_condition = delete_condition\n",
    "        self.update_condition = update_condition\n",
    "        self.insert_condition = insert_condition\n",
    "    \n",
    "\n",
    "    # Merge com operação apenas de DELETE (Ex: Delete de registros na tabela stage)\n",
    "    def merge_delete(self, source_df: DataFrame, stage_table: dict, custom_delete_condition: str = None) -> None:\n",
    "        target_delta_df = DeltaTable.forName(spark, stage_table[\"stage_name\"])\n",
    "        \n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), stage_table[\"merge_condition\"])\n",
    "                .whenMatchedDelete(custom_delete_condition)\n",
    "                .execute()\n",
    "        )\n",
    "    \n",
    "    # Merge somente com DELETE e UPDATE (Ex: Tabealas satélites que possuem algum tipo de filtro que qua quando alterado, é necessário deletear o registro na silver)\n",
    "    def merge_delete_update(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedDelete(self.delete_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Merge somente update com a tabela de destino (ex: update registros descritivos na silver)\n",
    "    def merge_update(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Merge somente com Insert e Update com a tabela de destino (ex: adicionando registros na tabela stage)\n",
    "    def merge_update_insert(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .whenNotMatchedInsert(\n",
    "                    condition = self.insert_condition,\n",
    "                    values = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Merge com todas as operações: DELETE, UDATE e INSERT\n",
    "    def merge_delete_update_insert(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedDelete(self.delete_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .whenNotMatchedInsert(\n",
    "                    condition = self.insert_condition,\n",
    "                    values = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "    \n",
    "    # Merge sem correspondencia com todas as operações: DELETE, UDATE e INSERT (Ex: merge com silver e delete de registros não correspondentes com DataFrame source)\n",
    "    def merge_unmatched_delete_update_insert(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .whenNotMatchedInsert(\n",
    "                    condition = self.insert_condition,\n",
    "                    values = self.column_merge)\n",
    "                .whenNotMatchedBySourceDelete()\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Criação de tabela\n",
    "    def create_table(self, columns_table:str, posfixo: str = '') -> None:\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {self.target_table_name}{posfixo} (\n",
    "                {columns_table}\n",
    "            )\n",
    "            TBLPROPERTIES (\n",
    "                delta.enableDeletionVectors = true,\n",
    "                delta.enableChangeDataFeed = true, \n",
    "                delta.autoOptimize.autoCompact=true,\n",
    "                delta.autoOptimize.optimizeWrite=true,\n",
    "                delta.columnMapping.mode = 'name',\n",
    "                delta.minReaderVersion = '3',\n",
    "                delta.minWriterVersion = '7',\n",
    "                delta.feature.allowColumnDefaults = 'supported'\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Set nome do owner da tabela para o grupo UnityAdmin\n",
    "        spark.sql(f\"ALTER TABLE {self.target_table_name}{posfixo} SET OWNER TO UnityAdmin\")\n",
    "        \n",
    "    # Tratamento de erro\n",
    "    def handling_exceptions(self, error) -> None:\n",
    "        type_error = str(error)\n",
    "            \n",
    "        if 'is not a Delta table' in type_error:\n",
    "            self.create_table()\n",
    "        else:\n",
    "            assert False, type_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18227cc4-65e6-4b79-a447-670a8a80b4e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Stream(Silver_table):\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, create_table_columns: str, column_merge: dict, merge_condition: str,  delete_condition: str, update_condition: str, insert_condition: str, table_id: str, checkpoint_location: str, query_name: str, merge_type: str, tvw_table_changes: str = None, join_query: str = None,  stage_table: dict = None, stream_filter = None, window_orderby: str = \"_commit_timestamp\", preimage_table: dict = None):\n",
    "        super().__init__(source_table_name, target_table_name, create_table_columns, column_merge, merge_condition, delete_condition, update_condition, insert_condition)\n",
    "        self.table_id = table_id\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.query_name = query_name\n",
    "        self.merge_type = merge_type\n",
    "        self.tvw_table_changes = tvw_table_changes\n",
    "        self.join_query = join_query\n",
    "        self.stage_table = stage_table\n",
    "        self.stream_filter = stream_filter\n",
    "        self.window_orderby = window_orderby\n",
    "        self.preimage_table = preimage_table\n",
    "        \n",
    "\n",
    "    # Transformações e Merge Streaming Table\n",
    "    def merge_changes(self, batch_df, batch_id) -> None:\n",
    "        # Filtro para registros mais recentes\n",
    "        window = Window.partitionBy(self.table_id).orderBy(col(self.window_orderby).desc())\n",
    "\n",
    "        batch_df = batch_df.withColumn(\"rank\", row_number().over(window)) \\\n",
    "                        .withColumnRenamed(\"_change_type\", \"_operation\") \\\n",
    "                        .where(\"rank = 1\") \\\n",
    "                        .drop(\"_commit_timestamp\", \"rank\")\n",
    "        \n",
    "\n",
    "        # Caso seja uma tabela que precise de join\n",
    "        if self.join_query is not None:\n",
    "            assert self.tvw_table_changes is not None, f'{datetime.datetime.now()} - WARNING - The variable \"tvw_table_changes: string\" cannot be empty'\n",
    "            batch_df.createOrReplaceTempView(self.tvw_table_changes)\n",
    "            batch_df = batch_df.sparkSession.sql(self.join_query)\n",
    "\n",
    "\n",
    "        \n",
    "        # Tente criar um datafrme delta da tabela de destino. Caso não consiga, crie a tabela e e crie o dataframe delta\n",
    "        try:\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        except Exception as error:\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            self.create_table(self.create_table_columns)\n",
    "            \n",
    "            # Cria um datafrme delta da tabela de destino recem criada\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        \n",
    "\n",
    "        # Merge com a tabela de destino\n",
    "        # satellite_table:\n",
    "        if self.merge_type == \"update\":\n",
    "            self.merge_update(target_delta_df, batch_df)\n",
    "        \n",
    "        # dependent_table\n",
    "        elif self.merge_type == \"delete_update\":\n",
    "            self.merge_delete_update(target_delta_df, batch_df)\n",
    "\n",
    "            # Delete registros da stage\n",
    "            if self.stage_table:                \n",
    "                self.merge_delete(batch_df, self.stage_table)\n",
    "        \n",
    "        # update_stage\n",
    "        elif self.merge_type == \"update_insert\":\n",
    "            self.merge_update_insert(target_delta_df, batch_df)\n",
    "        \n",
    "        elif self.merge_type == \"delete_update_insert\":\n",
    "            self.merge_delete_update_insert(target_delta_df, batch_df)\n",
    "        \n",
    "        # Caso não haja correspondência\n",
    "        else:\n",
    "            assert False, 'The \"merge_type\" variable (merge_type: str) was not filled in correctly. \\nCheck if it is among the following options: \\n\"update\"               -> to update only \\n\"delete_update\"        -> to delete and update \\n\"update_insert\"        -> to update and insert \\n\"delete_update_insert\" -> to delete, update and insert'\n",
    "    \n",
    "    # Transformações de regitros preimage e postimage\n",
    "    def tables_preimage_postimage(self, batch_df, batch_id) -> None:\n",
    "\n",
    "        # Tenta ler a stage de destino caso ela exista. Se não, ela será criada.\n",
    "        try:\n",
    "            # NOTA: Essa função ainda não foi necessária na criação na tabela silver. Descomentar caso for usar.\n",
    "            \"\"\"\n",
    "            preimage_delta_df = DeltaTable.forName(spark, self.target_table_name + \"_preimage\")\n",
    "            preimage_df = batch_df.filter(batch_df._change_type == \"update_preimage\")\n",
    "            \"\"\"\n",
    "\n",
    "            postimage_delta_df = DeltaTable.forName(spark, self.target_table_name + \"_postimage\")\n",
    "            postimage_df = batch_df.filter(batch_df._change_type.isin([\"update_postimage\", \"insert\", \"delete\"]))\n",
    "        except Exception as error:\n",
    "            # Criação da tabela stage PREIMAGE com valores _change_type = \"preimage\"\n",
    "            # NOTA: Essa função ainda não foi necessária na criação na tabela silver. Descomentar caso for usar.\n",
    "            \"\"\"\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            self.create_table(self.preimage_table['create_table_columns'], posfixo = \"_preimage\")\n",
    "\n",
    "            preimage_delta_df = DeltaTable.forName(spark, self.target_table_name + \"_preimage\")\n",
    "            preimage_df = batch_df.filter(batch_df._change_type == \"update_preimage\")\n",
    "            \"\"\"\n",
    "\n",
    "            # Criação da tabela stage POSTIMAGE com valores _change_type = \"postimage\"\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            \n",
    "            self.create_table(self.create_table_columns, posfixo = \"_postimage\")\n",
    "            postimage_delta_df = DeltaTable.forName(spark, self.target_table_name + \"_postimage\")\n",
    "            postimage_df = batch_df.filter(batch_df._change_type.isin([\"update_postimage\", \"insert\", \"delete\"]))\n",
    "\n",
    "            \n",
    "        # NOTA: Essa função ainda não foi necessária na criação na tabela silver. Descomentar caso for usar.\n",
    "        \"\"\"\n",
    "        window_pre = Window.partitionBy(self.table_id).orderBy(col(self.window_orderby).asc())\n",
    "        \"\"\"\n",
    "        window_pos = Window.partitionBy(self.table_id).orderBy(col(self.window_orderby).desc())\n",
    "\n",
    "        # NOTA: Essa função ainda não foi necessária na criação na tabela silver. Descomentar caso for usar.\n",
    "        \"\"\"\n",
    "        preimage_df = preimage_df.withColumn(\"rank\", row_number().over(window_pre)) \\\n",
    "                                .where(\"rank = 1\") \\\n",
    "                                .withColumnRenamed(\"_change_type\", \"_operation\") \\\n",
    "                                .drop(\"_commit_timestamp\", \"rank\")\n",
    "        \"\"\"\n",
    "\n",
    "        postimage_df = postimage_df.withColumn(\"rank\", row_number().over(window_pos)) \\\n",
    "                                .where(\"rank = 1\") \\\n",
    "                                .withColumnRenamed(\"_change_type\", \"_operation\") \\\n",
    "                                .drop(\"_commit_timestamp\", \"rank\")\n",
    "\n",
    "        # Verificação de except entre preimage e postimage\n",
    "        # NOTA: Essa função ainda não foi necessária na criação na tabela silver. Descomentar caso for usar.\n",
    "        '''\n",
    "        tvw_preimage_postimage = 'tvw_stage_deletes_' + self.target_table_name[self.target_table_name.rfind(\".\") + 1]\n",
    "        preimage_df.createOrReplaceTempView(f\"{tvw_preimage_postimage}_preimage\")\n",
    "        postimage_df.createOrReplaceTempView(f\"{tvw_preimage_postimage}_postimage\")\n",
    "        \n",
    "        preimage_df = preimage_df.sparkSession.sql(f\"\"\"\n",
    "            SELECT {self.preimage_table['except_columns']} FROM {tvw_preimage_postimage}_preimage\n",
    "            EXCEPT\n",
    "            SELECT {self.preimage_table['except_columns']} FROM {tvw_preimage_postimage}_postimage\n",
    "        \"\"\")\n",
    "        '''\n",
    "        \n",
    "        # update_stage\n",
    "        if self.merge_type == \"update_insert\":\n",
    "            # Merge de valores _change_type = \"postimage\"\n",
    "            if not postimage_df.isEmpty():\n",
    "                self.merge_update_insert(postimage_delta_df, postimage_df)\n",
    "\n",
    "            # Merge de valores _change_type = \"preimage\"\n",
    "            # NOTA: Essa função ainda não foi necessária na criação na tabela silver. Descomentar caso for usar.\n",
    "            \"\"\"\n",
    "            if not preimage_df.isEmpty():\n",
    "                preimage_df.display() # teste\n",
    "                self.column_merge = self.preimage_table['column_merge']\n",
    "                self.merge_update_insert(preimage_delta_df, preimage_df)\n",
    "            \"\"\"\n",
    "        # Caso não haja correspondência\n",
    "        else:\n",
    "            assert False, 'The \"merge_type\" variable (merge_type: str) was not filled in correctly. \\nCheck if it is among the following options: \\n\"update\"               -> to update only \\n\"delete_update\"        -> to delete and update \\n\"update_insert\"        -> to update and insert \\n\"delete_update_insert\" -> to delete, update and insert'\n",
    "\n",
    "    # Streaming Change Data Feed \n",
    "    def streaming_table_changes(self) -> None:\n",
    "        print(f'{datetime.datetime.now()} - INFO - Start Streaming Table Changes: {self.source_table_name}')\n",
    "\n",
    "        (\n",
    "            spark.readStream\n",
    "                .option(\"readChangeData\", True)\n",
    "                .table(self.source_table_name)\n",
    "                .filter(self.stream_filter)\n",
    "                .drop(\"_operation\")\n",
    "            .writeStream\n",
    "                .foreachBatch(self.merge_changes)\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_location}\")\n",
    "                .queryName(self.query_name)\n",
    "                .trigger(availableNow = True)\n",
    "                .outputMode(\"update\")\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "        )\n",
    "\n",
    "        print(f'{datetime.datetime.now()} - INFO - End Streaming Table Changes: {self.source_table_name}')\n",
    "\n",
    "    # Streaming Change Data Feed With Update_preimage \n",
    "    def streaming_table_pk_changes(self) -> None:\n",
    "        print(f'{datetime.datetime.now()} - INFO - Start Streaming Table Changes: {self.source_table_name}')\n",
    "\n",
    "        (\n",
    "            spark.readStream\n",
    "                .option(\"readChangeData\", True)\n",
    "                .table(self.source_table_name)\n",
    "                .filter(self.stream_filter)\n",
    "                .drop(\"_operation\")\n",
    "            .writeStream\n",
    "                .foreachBatch(self.tables_preimage_postimage)\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_location}\")\n",
    "                .queryName(self.query_name)\n",
    "                .trigger(availableNow = True)\n",
    "                .outputMode(\"update\")\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "        )\n",
    "\n",
    "        print(f'{datetime.datetime.now()} - INFO - End Streaming Table Changes: {self.source_table_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d629887e-8aca-4314-b026-ded288bb0da7",
     "showTitle": true,
     "title": "enrich join with stage"
    }
   },
   "outputs": [],
   "source": [
    "class Join_table(Silver_table):\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, create_table_columns: str, column_merge: dict, merge_condition: str, delete_condition: str, update_condition: str, insert_condition: str, stage_table: dict, join_query: str):\n",
    "        super().__init__(source_table_name, target_table_name, create_table_columns, column_merge, merge_condition,  delete_condition, update_condition, insert_condition)\n",
    "        self.stage_table = stage_table\n",
    "        self.join_query = join_query\n",
    "\n",
    "\n",
    "    # Função para criar um DataFrame baseado na coluna \"flag\" com registros a serem deletados\n",
    "    def delete_from_stage_with_flag(self, df: DataFrame) -> None:\n",
    "        # DataFrame somente com registros que serão deletados da stage, fazendo deduplicidade. \n",
    "        tvw_stage_deletes = 'tvw_stage_deletes_' + self.target_table_name[self.target_table_name.rfind(\".\") + 1]\n",
    "        df.createOrReplaceTempView(tvw_stage_deletes)  \n",
    "        \n",
    "        df = spark.sql(f\"\"\"\n",
    "                SELECT DISTINCT {self.stage_table[\"pk\"]}, _operation\n",
    "                FROM {tvw_stage_deletes}\n",
    "                WHERE flag = true OR (flag = false AND _operation = 'delete')\n",
    "            \"\"\")\n",
    "\n",
    "        self.merge_delete(df, self.stage_table)\n",
    "\n",
    "    \n",
    "    # Função de enriquecimento da tabela destino a partir de um join com uma tabela stage\n",
    "    def enrich_join_with_stage(self) -> None:\n",
    "\n",
    "        # Join com stage para enriquecer a tabela de destino\n",
    "        join_df = spark.sql(self.join_query)\n",
    "        \n",
    "        # Tente criar um dataframe delta da tabela de destino. Caso não consiga, crie a tabela e crie o dataframe delta\n",
    "        try:\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        except Exception as error:\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            self.create_table(self.create_table_columns)\n",
    "            \n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "\n",
    "        self.merge_delete_update_insert(target_delta_df, join_df)\n",
    "        self.delete_from_stage_with_flag(join_df)\n",
    "\n",
    "\n",
    "    # Função de enriquecimento da tabela destino a partir de um join com uma tabela stage com registros unificados\n",
    "    def enrich_join_with_stage_unified(self) -> None:\n",
    "        \n",
    "        # Join com stage para enriquecer a tabela de destino\n",
    "        join_df = spark.sql(self.join_query)\n",
    "\n",
    "        # Tente criar um datafrme delta da tabela de destino. Caso não consiga, crie a tabela e e crie o dataframe delta\n",
    "        try:\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        except Exception as error:\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            self.handling_exceptions(error)\n",
    "            \n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "\n",
    "        self.merge_delete_update_insert(target_delta_df, join_df)\n",
    "        self.merge_delete(join_df, self.stage_table)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "silver_table_accounting",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
