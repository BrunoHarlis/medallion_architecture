{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8415ec1-cd4b-40f7-b9a3-288f66faf58f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, current_timestamp, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from delta import DeltaTable\n",
    "import datetime\n",
    "\n",
    "# Config do spark para não processar lotes que não contenham dados\n",
    "spark.conf.set(\"spark.sql.streaming.noDataMicroBatches.enabled\", \"true\")\n",
    "\n",
    "# Config de evolução de schema na delta table\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db16bb8-f16e-42b9-bd2f-b4cb27619b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Silver_table:\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, create_table_columns: str, column_merge: dict, merge_condition: str,  delete_condition: str, update_condition: str, insert_condition: str):\n",
    "        self.source_table_name = source_table_name\n",
    "        self.target_table_name = target_table_name\n",
    "        self.create_table_columns = create_table_columns\n",
    "        self.column_merge = column_merge\n",
    "        self.merge_condition = merge_condition\n",
    "        self.delete_condition = delete_condition\n",
    "        self.update_condition = update_condition\n",
    "        self.insert_condition = insert_condition\n",
    "    \n",
    "\n",
    "    # Merge com operação apenas de DELETE (Ex: Delete de registros na tabela stage)\n",
    "    def merge_delete(self, source_df: DataFrame, stage_table: dict) -> None:\n",
    "        target_delta_df = DeltaTable.forName(spark, stage_table[\"stage_name\"])\n",
    "        \n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), stage_table[\"merge_condition\"])\n",
    "                .whenMatchedDelete(self.delete_condition)\n",
    "                .execute()\n",
    "        )\n",
    "    \n",
    "    # Merge somente com DELETE e UPDATE (Ex: Tabealas satélites que possuem algum tipo de filtro que qua quando alterado, é necessário deletear o registro na silver)\n",
    "    def merge_delete_update(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedDelete(self.delete_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Merge somente update com a tabela de destino (ex: update registros descritivos na silver)\n",
    "    def merge_update(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Merge somente com Insert e Update com a tabela de destino (ex: adicionando registros na tabela stage)\n",
    "    def merge_update_insert(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .whenNotMatchedInsert(\n",
    "                    condition = self.insert_condition,\n",
    "                    values = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Merge com todas as operações: DELETE, UDATE e INSERT\n",
    "    def merge_delete_update_insert(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedDelete(self.delete_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .whenNotMatchedInsert(\n",
    "                    condition = self.insert_condition,\n",
    "                    values = self.column_merge)\n",
    "                .execute()\n",
    "        )\n",
    "    \n",
    "    # Merge sem correspondencia com todas as operações: DELETE, UDATE e INSERT (Ex: merge com silver e delete de registros não correspondentes com DataFrame source)\n",
    "    def merge_unmatched_delete_update_insert(self, target_delta_df: DeltaTable, source_df: DataFrame) -> None:\n",
    "        (\n",
    "            target_delta_df.alias(\"t\")\n",
    "                .merge(source_df.alias(\"s\"), self.merge_condition)\n",
    "                .whenMatchedUpdate(\n",
    "                    condition = self.update_condition,\n",
    "                    set = self.column_merge)\n",
    "                .whenNotMatchedInsert(\n",
    "                    condition = self.insert_condition,\n",
    "                    values = self.column_merge)\n",
    "                .whenNotMatchedBySourceDelete()\n",
    "                .execute()\n",
    "        )\n",
    "\n",
    "    # Criação de tabela\n",
    "    def create_table(self) -> None:\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {self.target_table_name} (\n",
    "                {self.create_table_columns}\n",
    "            )\n",
    "            TBLPROPERTIES (\n",
    "                delta.enableDeletionVectors = true,\n",
    "                delta.enableChangeDataFeed = true, \n",
    "                delta.autoOptimize.autoCompact=true,\n",
    "                delta.autoOptimize.optimizeWrite=true,\n",
    "                delta.columnMapping.mode = 'name',\n",
    "                delta.minReaderVersion = '3',\n",
    "                delta.minWriterVersion = '7',\n",
    "                delta.feature.allowColumnDefaults = 'supported'\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Set nome do owner da tabela para o grupo UnityAdmin\n",
    "        spark.sql(f\"ALTER TABLE {self.target_table_name} SET OWNER TO UnityAdmin\")\n",
    "    \n",
    "    # Tratamento de erro\n",
    "    def handling_exceptions(self, error) -> None:\n",
    "        type_error = str(error)\n",
    "            \n",
    "        if 'is not a Delta table' in type_error:\n",
    "            self.create_table()\n",
    "        else:\n",
    "            assert False, type_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8399716d-1c5e-4cac-84e9-fec77efafd9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Stream(Silver_table):\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, create_table_columns: str, column_merge: dict, merge_condition: str,  delete_condition: str, update_condition: str, insert_condition: str, table_id: str, checkpoint_location: str, query_name: str, merge_type: str, tvw_table_changes: str = None, join_query: str = None,  stage_table: dict = None, stream_filter = None, window_orderby: str = \"_commit_timestamp\"):\n",
    "        super().__init__(source_table_name, target_table_name, create_table_columns, column_merge, merge_condition, delete_condition, update_condition, insert_condition)\n",
    "        self.table_id = table_id\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.query_name = query_name\n",
    "        self.merge_type = merge_type\n",
    "        self.tvw_table_changes = tvw_table_changes\n",
    "        self.join_query = join_query\n",
    "        self.stage_table = stage_table\n",
    "        self.stream_filter = stream_filter\n",
    "        self.window_orderby = window_orderby\n",
    "        \n",
    "\n",
    "    # Transformações e Merge Streaming Table\n",
    "    def merge_changes(self, batch_df, batch_id):\n",
    "        # Filtro para registros mais recentes\n",
    "        window = Window.partitionBy(self.table_id).orderBy(col(self.window_orderby).desc())\n",
    "        batch_df = batch_df.withColumn(\"rank\", row_number().over(window)) \\\n",
    "                        .withColumn(\"_silverUpdateDate\", current_timestamp()) \\\n",
    "                        .withColumnRenamed(\"_change_type\", \"_operation\") \\\n",
    "                        .where(\"rank = 1\") \\\n",
    "                        .drop(\"_commit_timestamp\", \"rank\")\n",
    "        \n",
    "\n",
    "        # Caso seja uma tabela que precise de join\n",
    "        if self.join_query is not None:\n",
    "            assert self.tvw_table_changes is not None, f'{datetime.datetime.now()} - WARNING - The variable \"tvw_table_changes: string\" cannot be empty'\n",
    "\n",
    "            batch_df.createOrReplaceTempView(self.tvw_table_changes)\n",
    "            batch_df = batch_df.sparkSession.sql(self.join_query)\n",
    "\n",
    "        \n",
    "        # Tente criar um datafrme delta da tabela de destino. Caso não consiga, crie a tabela e e crie o dataframe delta\n",
    "        try:\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        except Exception as error:\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            self.handling_exceptions(error)\n",
    "            \n",
    "            # Cria um datafrme delta da tabela de destino recem criada\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        \n",
    "\n",
    "        # Merge com a tabela de destino\n",
    "        # satellite_table:\n",
    "        if self.merge_type == \"update\":\n",
    "            self.merge_update(target_delta_df, batch_df)\n",
    "        \n",
    "        # dependent_table\n",
    "        elif self.merge_type == \"delete_update\":\n",
    "            self.merge_delete_update(target_delta_df, batch_df)\n",
    "\n",
    "            # Delete registros da stage\n",
    "            if self.stage_table:                \n",
    "                self.merge_delete(batch_df, self.stage_table)\n",
    "        \n",
    "        # update_stage\n",
    "        elif self.merge_type == \"update_insert\":\n",
    "            self.merge_update_insert(target_delta_df, batch_df)\n",
    "        \n",
    "        elif self.merge_type == \"delete_update_insert\":\n",
    "            self.merge_delete_update_insert(target_delta_df, batch_df)\n",
    "        \n",
    "        # Caso não haja correspondência\n",
    "        else:\n",
    "            assert False, 'The \"merge_type\" variable (merge_type: str) was not filled in correctly. \\nCheck if it is among the following options: \\n\"update\"               -> to update only \\n\"delete_update\"        -> to delete and update \\n\"update_insert\"        -> to update and insert \\n\"delete_update_insert\" -> to delete, update and insert'\n",
    "    \n",
    "    # Streaming Change Data Feed \n",
    "    def streaming_table_changes(self):\n",
    "        print(f'{datetime.datetime.now()} - INFO - Start Streaming Table Changes: {self.source_table_name}')\n",
    "\n",
    "        (\n",
    "            spark.readStream\n",
    "                .option(\"readChangeData\", True)\n",
    "                .table(self.source_table_name)\n",
    "                .filter(self.stream_filter)\n",
    "                .drop(\"_operation\")\n",
    "            .writeStream\n",
    "                .foreachBatch(self.merge_changes)\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_location}\")\n",
    "                .queryName(self.query_name)\n",
    "                .trigger(availableNow = True)\n",
    "                .outputMode(\"update\")\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "        )\n",
    "\n",
    "        print(f'{datetime.datetime.now()} - INFO - End Streaming Table Changes: {self.source_table_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1ade9d4-748d-4e97-af36-88da17edd1a7",
     "showTitle": true,
     "title": "enrich join with stage"
    }
   },
   "outputs": [],
   "source": [
    "class Join_table(Silver_table):\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, create_table_columns: str, column_merge: dict, merge_condition: str, delete_condition: str, update_condition: str, insert_condition: str, stage_table: dict, join_query: str):\n",
    "        super().__init__(source_table_name, target_table_name, create_table_columns, column_merge, merge_condition,  delete_condition, update_condition, insert_condition)\n",
    "        self.stage_table = stage_table\n",
    "        self.join_query = join_query\n",
    "\n",
    "\n",
    "    # Função para criar um DataFrame baseado na coluna \"flag\" com registros a serem deletados\n",
    "    def delete_from_stage_with_flag(self, df: DataFrame) -> None:\n",
    "        # DataFrame somente com registros que serão deletados da stage, fazendo deduplicidade. \n",
    "        tvw_stage_deletes = 'tvw_stage_deletes_' + self.target_table_name[self.target_table_name.rfind(\".\") + 1]\n",
    "        df.createOrReplaceTempView(tvw_stage_deletes)  \n",
    "        \n",
    "        df = spark.sql(f\"\"\"\n",
    "                SELECT DISTINCT {self.stage_table[\"pk\"]}, _operation\n",
    "                FROM {tvw_stage_deletes}\n",
    "                WHERE flag = true OR (flag = false AND _operation = 'delete')\n",
    "            \"\"\")\n",
    "\n",
    "        self.merge_delete(df, self.stage_table)\n",
    "\n",
    "    # Função de enriquecimento da tabela destino a partir de um join com uma tabela stage\n",
    "    def enrich_join_with_stage(self) -> None:\n",
    "\n",
    "        # Join com stage para enriquecer a tabela de destino\n",
    "        join_df = spark.sql(self.join_query)\n",
    "        \n",
    "        # Tente criar um datafrme delta da tabela de destino. Caso não consiga, crie a tabela e e crie o dataframe delta\n",
    "        try:\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        except Exception as error:\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            self.handling_exceptions(error)\n",
    "            \n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "\n",
    "\n",
    "        self.merge_delete_update_insert(target_delta_df, join_df)\n",
    "        self.delete_from_stage_with_flag(join_df)\n",
    "\n",
    "    # Função de enriquecimento da tabela destino a partir de um join com uma tabela stage com registros unificados\n",
    "    def enrich_join_with_stage_unified(self) -> None:\n",
    "        \n",
    "        # Join com stage para enriquecer a tabela de destino\n",
    "        join_df = spark.sql(self.join_query)\n",
    "\n",
    "        # Tente criar um datafrme delta da tabela de destino. Caso não consiga, crie a tabela e e crie o dataframe delta\n",
    "        try:\n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "        except Exception as error:\n",
    "            assert self.create_table_columns is not None, f'{datetime.datetime.now()} - WARNING - The target table does not exist! Set the variable \"create_table_columns: str\"'\n",
    "            self.handling_exceptions(error)\n",
    "            \n",
    "            target_delta_df = DeltaTable.forName(spark, self.target_table_name)\n",
    "\n",
    "        self.merge_delete_update_insert(target_delta_df, join_df)\n",
    "        self.merge_delete(join_df, self.stage_table)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
